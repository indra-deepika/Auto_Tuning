{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate random data for various workloads ( for Random Forest and feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_metric(file_path = './db_bench_output.txt'):\n",
    "    metrics = {}\n",
    "    micros_op_pattern = re.compile(r\"(\\d+\\.\\d+) micros/op (\\d+) ops/sec\")\n",
    "    write_read_rate_pattern = re.compile(r\"Write rate: (\\d+) bytes/second\\nRead rate: (\\d+) ops/second\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "        \n",
    "        micros_op_match = micros_op_pattern.search(file_content)\n",
    "        if micros_op_match:\n",
    "            metrics['micros_per_op'] = float(micros_op_match.group(1))\n",
    "            metrics['ops_per_sec'] = int(micros_op_match.group(2))\n",
    "            \n",
    "\n",
    "        write_read_rate_match = write_read_rate_pattern.search(file_content)\n",
    "        if write_read_rate_match:\n",
    "            metrics['write_rate'] = int(write_read_rate_match.group(1))\n",
    "            metrics['read_rate'] = int(write_read_rate_match.group(2))\n",
    "\n",
    "    return   metrics['ops_per_sec'] , metrics['micros_per_op']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter ranges\n",
    "parameter_ranges = {\n",
    "    'max_background_compactions': [1, 8],\n",
    "    'max_background_flushes': [1, 8],\n",
    "    'write_buffer_size': [4*1024*1024, 1024*1024*1024],\n",
    "    'max_write_buffer_number': [2, 8],\n",
    "    'min_write_buffer_number_to_merge': [1, 5],\n",
    "    'max_bytes_for_level_multiplier': [2, 16],\n",
    "    'block_size': [4*1024, 128*1024],\n",
    "    'level0_file_num_compaction_trigger': [2, 16],\n",
    "    'level0_slowdown_writes_trigger': [2, 32],\n",
    "    'level0_stop_writes_trigger': [2, 32],\n",
    "    'target_file_size_multiplier': [1, 8], \n",
    "    'target_file_size_base': [33554432, 134217728]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "# Now you can import the module\n",
    "import rocksdb_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RocksDB:    version 8.10.0\n",
      "Date:       Wed May 29 18:17:26 2024\n",
      "CPU:        12 * Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz\n",
      "CPUCache:   12288 KB\n",
      "RocksDB:    version 8.10.0                           \n",
      "Date:       Wed May 29 18:17:36 2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_workload\n",
      "recommendations\n",
      "cmd: /mnt/c/Users/edeep/Final_Rocksdb/rocksdb/db_bench --benchmarks=fillseq --num=100000 --compression_type=none --key_size=1024 --value_size=10240 --block_size=15833 --level0_file_num_compaction_trigger=15 --level0_slowdown_writes_trigger=11 --level0_stop_writes_trigger=21 --max_background_compactions=5 --max_background_flushes=3 --max_bytes_for_level_multiplier=15 --max_write_buffer_number=3 --min_write_buffer_number_to_merge=5 --target_file_size_base=61402544 --target_file_size_multiplier=4 --write_buffer_size=594934666100736\n",
      "ops: 12000, micro: 83.205\n",
      "run_workload\n",
      "recommendations\n",
      "cmd: /mnt/c/Users/edeep/Final_Rocksdb/rocksdb/db_bench --benchmarks=fillseq --num=100000 --compression_type=none --key_size=1024 --value_size=10240 --block_size=22040 --level0_file_num_compaction_trigger=16 --level0_slowdown_writes_trigger=4 --level0_stop_writes_trigger=18 --max_background_compactions=5 --max_background_flushes=1 --max_bytes_for_level_multiplier=8 --max_write_buffer_number=8 --min_write_buffer_number_to_merge=5 --target_file_size_base=55666314 --target_file_size_multiplier=2 --write_buffer_size=362613694267392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CPU:        12 * Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz\n",
      "CPUCache:   12288 KB\n",
      "... finished 100000 ops                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ops: 11858, micro: 84.33\n",
      "run_workload\n",
      "recommendations\n",
      "cmd: /mnt/c/Users/edeep/Final_Rocksdb/rocksdb/db_bench --benchmarks=fillseq --num=100000 --compression_type=none --key_size=1024 --value_size=10240 --block_size=109654 --level0_file_num_compaction_trigger=5 --level0_slowdown_writes_trigger=20 --level0_stop_writes_trigger=2 --max_background_compactions=5 --max_background_flushes=6 --max_bytes_for_level_multiplier=10 --max_write_buffer_number=7 --min_write_buffer_number_to_merge=2 --target_file_size_base=96899329 --target_file_size_multiplier=4 --write_buffer_size=905678295138304\n",
      "ops: 11106, micro: 90.033\n",
      "Data generation complete. The dataset is appended to './rocksdb_performance_data.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RocksDB:    version 8.10.0\n",
      "Date:       Wed May 29 18:17:46 2024\n",
      "CPU:        12 * Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz\n",
      "CPUCache:   12288 KB\n",
      "... finished 100000 ops                              \r"
     ]
    }
   ],
   "source": [
    "# Function to generate random samples within the given ranges and collect performance metrics\n",
    "def generate_random_samples(n_samples=1500, parameter_ranges=parameter_ranges, csv_file='./rocksdb_performance_data.csv'):\n",
    "    # Check if the CSV file exists, if not, write the header\n",
    "    if not os.path.exists(csv_file):\n",
    "        with open(csv_file, mode='w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=list(parameter_ranges.keys()) + ['ops_per_sec', 'micros_per_op'])\n",
    "            writer.writeheader()\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        samples = {}\n",
    "        for param, (low, high) in parameter_ranges.items():\n",
    "            if isinstance(low, int) and isinstance(high, int):\n",
    "                samples[param] = np.random.randint(low, high + 1)\n",
    "            else:\n",
    "                samples[param] = np.random.uniform(low, high)\n",
    "        \n",
    "        wltype = \"fillseq\" # change workload as necessary \n",
    "\n",
    "        num_operations = 100000\n",
    "        db_path = \"\"  # Ensure this is correctly set or managed in your actual use case\n",
    "\n",
    "        # Load the workload on the rocksdb_module\n",
    "        rec_temp = {key: str(value) for key, value in samples.items()}\n",
    "        other_params = {}\n",
    "        rocksdb_module.run_workload(wltype, num_operations, db_path, rec_temp, other_params)\n",
    "\n",
    "        # Read the operations and microseconds from the metric function\n",
    "        ops, micro = read_metric()\n",
    "        print(f\"ops: {ops}, micro: {micro}\")\n",
    "\n",
    "        # Add the performance metrics to the sample\n",
    "        samples['ops_per_sec'] = ops\n",
    "        samples['micros_per_op'] = micro\n",
    "        \n",
    "        # Append the sample to the CSV file\n",
    "        with open(csv_file, mode='a', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=samples.keys())\n",
    "            writer.writerow(samples)\n",
    "\n",
    "    print(f\"Data generation complete. The dataset is appended to '{csv_file}'.\")\n",
    "\n",
    "# Generate the dataset and keep appending data to the CSV file\n",
    "generate_random_samples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
